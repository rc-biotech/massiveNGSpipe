% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pipeline_config.R
\name{pipeline_config}
\alias{pipeline_config}
\title{Initial config setup}
\usage{
pipeline_config(
  project_dir = file.path(dirname(config)[1], "NGS_pipeline"),
  config = ORFik::config(),
  complete_metadata = file.path(project_dir, "FINAL_LIST.csv"),
  backup_metadata = file.path(project_dir, "BACKUP_LIST.csv"),
  temp_metadata = file.path(project_dir, "next_round_manual.csv"),
  blacklist = file.path(project_dir, "BLACKLIST.csv"),
  google_url = default_sheets(project_dir),
  preset = "Ribo-seq",
  flags = pipeline_flags(project_dir, mode, preset, contam),
  flag_steps = flag_grouping(flags),
  pipeline_steps = lapply(names(flag_steps), function(x) get(x, mode = "function")),
  mode = c("online", "local")[1],
  contam = FALSE,
  delete_raw_files = mode == "online",
  delete_trimmed_files = mode == "online",
  delete_collapsed_files = mode == "online",
  keep_contaminants = FALSE,
  keep_unaligned_genome = FALSE,
  compress_raw_data = FALSE,
  stop_downloading_new_data_at_drive_usage = 96,
  max_unprocessed_downloads = 30,
  accepted_lengths_rpf = c(20, 21, 25:33),
  reuse_shifts_if_existing = TRUE,
  split_unique_mappers = FALSE,
  all_mappers = TRUE,
  parallel_conf = bpoptions(log = TRUE, jobname = "pipeline_step", logdir =
    file.path(project_dir, "log_pipeline"), stop.on.error = TRUE),
  discord_webhook = massiveNGSpipe:::discord_connection_default_cached(),
  verbose = TRUE,
  BPPARAM_MAIN = BiocParallel::MulticoreParam(length(pipeline_steps)),
  BPPARAM_TRIM = BiocParallel::MulticoreParam(min(BiocParallel::bpworkers(), 8)),
  BPPARAM = bpparam()
)
}
\arguments{
\item{project_dir}{where will specific pipeline outputs be put. Default:
file.path(dirname(config)\link{1}, "NGS_pipeline").
If you need seperat pipelines, please use different locations.}

\item{config}{path, default \code{ORFik::config()}, where will
fastq, bam, references and ORFik experiments go}

\item{complete_metadata}{path, default: file.path(project_dir, "FINAL_LIST.csv")
Where should completed valid metadata be stored as csv?}

\item{backup_metadata}{path, default: file.path(project_dir, "BACKUP_LIST.csv").
The complete list of all unique runs checked in this pipelines,
even ones deleted earlier. Useful for check of what has been done before.}

\item{temp_metadata}{path, default: file.path(project_dir, "next_round_manual.csv").
The intermediate file used after curation, but before it is validated. This is
the where you update the current new metadata annotations for final approval into the
complete_metadata. This syncs automatically to the google_url sheet if included.}

\item{blacklist}{path, Which studies to ignore for this config,
default: file.path(project_dir, "BLACKLIST.csv")\cr
A csv with 1 column id, which gives BioProject IDs}

\item{google_url}{url or sheet object for google sheet to use. Set to NULL to
not use google sheet.}

\item{preset}{character, default "Ribo-seq".
Alternatives: c("Ribo-seq", "RNA-seq", "disome", "empty")}

\item{flags}{named character vector, with cut points, where can
the pipeline continue if it breaks? Is defined in combination with
'steps' argument that defines that actuall function called for each break point.
Increasing break points makes the pipeline run faster at the cost of more
resource usage.}

\item{flag_steps}{list, mapping of functions to ids. The names per list element is a function.
The character elements inside each list element are the flag name ids for all steps that will
be "marked as done" inside that function. Example: In default preset,
the pipe_trim_collapse function marks as done both the trim and collapse flags.}

\item{pipeline_steps}{a list of the functions to actually run, the functions
must be named equal to names of the 'flag_step' argument.}

\item{mode}{= \code{c("online", "local")[1]}. "online" will assume project IDs for
online repository (SRA, ENA, PRJ etc). Local means local folders as accessions.}

\item{contam}{logical, FALSE, do contamint removal, using a seperat made
STAR index during genome preparation step.}

\item{delete_raw_files}{logical, default: mode == "online". If online do delete
raw fastq files after trim step is done, for local samples do not delete.
Set only to TRUE for mode local, if you have backups!}

\item{delete_trimmed_files}{logical, default: mode == "online".
If TRUE deletes the trimmed fasta files.}

\item{delete_collapsed_files}{logical, default: mode == "online".
If TRUE deletes the collapsed fasta files.}

\item{keep_contaminants}{logical, default FALSE. Do not keep contaminant aligned reads,
if TRUE they are saved in contamination dir. Ignored if "contam" is not in flags to use.}

\item{keep_unaligned_genome}{logical, default FALSE. Do not keep contaminant aligned reads,
else saved in contamination dir.}

\item{compress_raw_data}{logical, default FALSE. If TRUE, will compress raw fastq files.}

\item{stop_downloading_new_data_at_drive_usage}{integer, default 96,
percentage value where the drive will stop downloading new data. Set to 101 to
disable a cap.}

\item{max_unprocessed_downloads}{numeric, default 30. Temporarily stop downloading more data
if > 30 studies are not done with the first post download processing step
(usually trimming)}

\item{accepted_lengths_rpf}{default c(20, 21, 25:33), which read lengths to pshift.
Default is the standard fractions of normal 80S ~28 and the smaller size of ~21.}

\item{reuse_shifts_if_existing}{for Ribo-seq, reuse shift table called shifting_table.rds
in pshifted folder if it is valid (equal number of sample shift tables in file
relative toexperiment)}

\item{split_unique_mappers}{logical, default FALSE.
Run for unique mappers only, split out into seperate directory.}

\item{all_mappers}{logical, default TRUE. Run for all mappers}

\item{parallel_conf}{a bpoptions object, default:
\code{bpoptions(log =TRUE,
 jobname = "pipeline_step",
 logdir = file.path(project_dir, "log_pipeline"),
 stop.on.error = TRUE)}
Specific pipeline config for parallel settings and log directory for BPPARAM_MAIN}

\item{discord_webhook}{= massiveNGSpipe:::discord_connection_default_cached()}

\item{verbose}{logical, default TRUE, give start up message}

\item{BPPARAM_MAIN}{BiocParallel::MulticoreParam(length(pipeline_steps))
The main parallel backend for pipeline, specifying logging behavoir etc.}

\item{BPPARAM_TRIM}{BiocParallel::MulticoreParam(max(BiocParallel::bpworkers(), 8)),
number of cores/threads to use for trimming. Optimal is 8 for most data.}

\item{BPPARAM}{= bpparam(), number of cores/threads to use.}
}
\value{
a list with a defined config
}
\description{
Set up all paths, functions to be run and flag directories.
Also adds parallel processing settings and google integration.
}
